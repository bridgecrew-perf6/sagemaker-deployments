{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "prefix = \"sagemaker-datasetprep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: dbpedia_csv/test.csv: Cannot change ownership to uid 1000, gid 1000: Operation not permitted\n",
      "tar: dbpedia_csv/classes.txt: Cannot change ownership to uid 1000, gid 1000: Operation not permitted\n",
      "tar: dbpedia_csv/train.csv: Cannot change ownership to uid 1000, gid 1000: Operation not permitted\n",
      "tar: dbpedia_csv/readme.txt: Cannot change ownership to uid 1000, gid 1000: Operation not permitted\n",
      "tar: dbpedia_csv: Cannot change ownership to uid 1000, gid 1000: Operation not permitted\n",
      "tar: Exiting with failure status due to previous errors\n"
     ]
    }
   ],
   "source": [
    "!tar -xzf dbpedia_csv.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,\"E. D. Abbott Ltd\",\" Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.\"\n",
      "1,\"Schwan-Stabilo\",\" Schwan-STABILO is a German maker of pens for writing colouring and cosmetics as well as markers and highlighters for office use. It is the world's largest manufacturer of highlighter pens Stabilo Boss.\"\n",
      "1,\"Q-workshop\",\" Q-workshop is a Polish company located in Poznań that specializes in designand production of polyhedral dice and dice accessories for use in various games (role-playing gamesboard games and tabletop wargames). They also run an online retail store and maintainan active forum community.Q-workshop was established in 2001 by Patryk Strzelewicz – a student from Poznań. Initiallythe company sold its products via online auction services but in 2005 a website and online store wereestablished.\"\n"
     ]
    }
   ],
   "source": [
    "!head dbpedia_csv/train.csv -n 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,\"Automatic Electric\",\" Automatic Electric Company (AE) was the largest of the manufacturing units of the Automatic Electric Group. It was a telephone equipment supplier for independent telephone companies in North America and also had a world-wide presence. With its line of automatic telephone exchanges it was also a long-term supplier of switching equipment to the Bell System starting in 1919.\"\n",
      "1,\"Tokyo Marui\",\" Tokyo Marui Co. Ltd (株式会社東京マルイ Kabushiki-gaisha Tōkyō Marui) is an airsoft gun manufacturer located in Adachi Tokyo Japan. They are best known for creating the AEG(Automatic electric gun). Its main market is Japan but third-party retailers sell in Hong Kong (PRC) Taiwan (ROC) South Korea East Asia and worldwide. Such is the popularity of its guns that the company has its own center for airsoft sport called Tokyo Marui BB Sports Field.\"\n"
     ]
    }
   ],
   "source": [
    "!grep -i \"automatic electric\"  dbpedia_csv/train.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company\n",
      "EducationalInstitution\n",
      "Artist\n",
      "Athlete\n",
      "OfficeHolder\n",
      "MeanOfTransportation\n",
      "Building\n",
      "NaturalPlace\n",
      "Village\n",
      "Animal\n",
      "Plant\n",
      "Album\n",
      "Film\n",
      "WrittenWork\n"
     ]
    }
   ],
   "source": [
    "!cat dbpedia_csv/classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 'Company', '2': 'EducationalInstitution', '3': 'Artist', '4': 'Athlete', '5': 'OfficeHolder', '6': 'MeanOfTransportation', '7': 'Building', '8': 'NaturalPlace', '9': 'Village', '10': 'Animal', '11': 'Plant', '12': 'Album', '13': 'Film', '14': 'WrittenWork'}\n"
     ]
    }
   ],
   "source": [
    "d_label = {}\n",
    "with open ('dbpedia_csv/classes.txt') as f:\n",
    "    for i, label in enumerate(f.readlines()):\n",
    "        d_label[str(i + 1)] = label.strip()\n",
    "print(d_label)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(row):\n",
    "    cur_row = []\n",
    "    label = f'__label__{d_label[row[0]]}'\n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(row[1].lower()))\n",
    "    cur_row.extend(nltk.word_tokenize(row[2].lower()))\n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "\n",
    "def preprocess(input_file, output_file, keep=1):\n",
    "    all_rows = []\n",
    "    with open(input_file, 'r') as csvinfile:\n",
    "        csv_reader = csv.reader(csvinfile, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            all_rows.append(row)\n",
    "    shuffle(all_rows)\n",
    "    all_rows = all_rows[: int(keep * len(all_rows))]\n",
    "    pool = Pool(processes = multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_text, all_rows)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csvwriter = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        csvwriter.writerows(transformed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.59 s, sys: 1.18 s, total: 9.77 s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preprocess('dbpedia_csv/train.csv', 'dbpedia_csv/dbpedia.train', keep=0.2)\n",
    "preprocess('dbpedia_csv/test.csv', 'dbpedia_csv/dbpedia.validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__Building ackerman house ( saddle river new jersey ) ackerman house is located in saddle river bergen county new jersey united states . the house was built in 1811 and was added to the national register of historic places on january 10 1983 .\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 dbpedia_csv/dbpedia.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:1\n"
     ]
    }
   ],
   "source": [
    "image=sagemaker.image_uris.retrieve(framework='blazingtext', \n",
    "                                    region=region, \n",
    "                                    version='1')\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = f's3://{bucket}/{prefix}/output'\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "            image,\n",
    "            role,\n",
    "            instance_count=1,\n",
    "            instance_type='ml.c5.2xlarge',\n",
    "            volume_size=30,\n",
    "            max_run=360000,\n",
    "            input_mode='File',\n",
    "            enable_sagemaker_metrics=True,\n",
    "            output_path=s3_output_location,\n",
    "            hyperparameters={\n",
    "                'mode': 'supervised',\n",
    "                'epochs': 20,\n",
    "                'min_count': 2,\n",
    "                'learning_rate': 0.05,\n",
    "                'vector_dim': 10,\n",
    "                'early_stopping': True,\n",
    "                'patience': 4,\n",
    "                'min_epochs': 5,\n",
    "                'word_ngrams': 2,\n",
    "            },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-104877823522/sagemaker-datasetprep/train\n",
      "s3://sagemaker-us-east-1-104877823522/sagemaker-datasetprep/validation\n",
      "2022-04-11 07:17:10 Starting - Starting the training job...\n",
      "2022-04-11 07:17:34 Starting - Preparing the instances for trainingProfilerReport-1649661430: InProgress\n",
      "......\n",
      "2022-04-11 07:18:37 Downloading - Downloading input data...\n",
      "2022-04-11 07:19:08 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[04/11/2022 07:19:10 WARNING 139927612053312] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[04/11/2022 07:19:10 WARNING 139927612053312] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[04/11/2022 07:19:10 INFO 139927612053312] nvidia-smi took: 0.025180816650390625 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[04/11/2022 07:19:10 INFO 139927612053312] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34mNumber of CPU sockets found in instance is  1\u001b[0m\n",
      "\u001b[34m[04/11/2022 07:19:10 INFO 139927612053312] Processing /opt/ml/input/data/train/dbpedia.train . File size: 35.00918006896973 MB\u001b[0m\n",
      "\u001b[34m[04/11/2022 07:19:10 INFO 139927612053312] Processing /opt/ml/input/data/validation/dbpedia.validation . File size: 21.88746166229248 MB\u001b[0m\n",
      "\u001b[34mRead 6M words\u001b[0m\n",
      "\u001b[34mNumber of words:  148479\u001b[0m\n",
      "\u001b[34mLoading validation data from /opt/ml/input/data/validation/dbpedia.validation\u001b[0m\n",
      "\u001b[34mLoaded validation data.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0442  Progress: 11.52%  Million Words/sec: 19.54 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0417  Progress: 16.63%  Million Words/sec: 20.03 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 3\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0392  Progress: 21.67%  Million Words/sec: 20.22 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 4\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 5\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.969171\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0364  Progress: 27.15%  Million Words/sec: 18.12 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 6\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.971943\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0335  Progress: 32.93%  Million Words/sec: 16.78 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 7\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.973057\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0309  Progress: 38.12%  Million Words/sec: 15.89 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 8\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.973943\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0281  Progress: 43.84%  Million Words/sec: 15.33 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 9\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.974971\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0253  Progress: 49.34%  Million Words/sec: 14.94 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 10\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.975029\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0227  Progress: 54.58%  Million Words/sec: 14.64 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 11\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.975357\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0199  Progress: 60.27%  Million Words/sec: 14.46 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 12\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.975371\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0172  Progress: 65.68%  Million Words/sec: 14.27 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 13\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.975971\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0143  Progress: 71.35%  Million Words/sec: 14.10 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 14\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.976214\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 15\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.976086\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0115  Progress: 76.92%  Million Words/sec: 13.63 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 16\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.976443\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0087  Progress: 82.69%  Million Words/sec: 13.51 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 17\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.9764\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0059  Progress: 88.23%  Million Words/sec: 13.53 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 18\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.976743\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0033  Progress: 93.49%  Million Words/sec: 13.44 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 19\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.976629\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0007  Progress: 98.63%  Million Words/sec: 13.43 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 20\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.9765\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 13.38 #####\u001b[0m\n",
      "\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 13.38\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 9.31\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.9962\u001b[0m\n",
      "\u001b[34mNumber of train examples: 112000\u001b[0m\n",
      "\u001b[34m#validation_accuracy: 0.9767\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 70000\u001b[0m\n",
      "\n",
      "2022-04-11 07:19:35 Uploading - Uploading generated training model\n",
      "2022-04-11 07:19:55 Completed - Training job completed\n",
      "Training seconds: 72\n",
      "Billable seconds: 72\n"
     ]
    }
   ],
   "source": [
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='dbpedia_csv/dbpedia.train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='dbpedia_csv/dbpedia.validation', bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = f's3://{bucket}/{train_channel}'\n",
    "s3_validation_data = f's3://{bucket}/{validation_channel}'\n",
    "print(s3_train_data)\n",
    "print(s3_validation_data)\n",
    "\n",
    "data_channels = {'train': s3_train_data, \n",
    "                 'validation': s3_validation_data}\n",
    "\n",
    "exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "jobname = f'dbpedia-blazingtext-{exp_datetime}'\n",
    "\n",
    "estimator.fit(inputs=data_channels,\n",
    "              job_name=jobname,\n",
    "              logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
